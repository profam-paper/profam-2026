# @package _global_
# Preset: replicate the ProFam-1 preprint configuration.
# Base model/trainer/callbacks/logger/tokenizer/constants are defined in `configs/train.yaml`.
# The FunFams dataset here is depricated in favour of the ted datset defined in data/profam.yaml

model:
  config:
    _target_: transformers.LlamaConfig
    vocab_size: ${constants.vocab_size}
    hidden_size: 1024
    intermediate_size: 4096
    num_attention_heads: 16
    num_hidden_layers: 16
    num_key_value_heads: 8
    rope_theta: 500000
    max_position_embeddings: 131072
    scoring_max_tokens: 128000
    attn_implementation: flash_attention_2
    attention_bias: false
    attention_dropout: 0.0
    rms_norm_eps: 1.0e-05
    hidden_act: silu
    torch_dtype: bfloat16
    use_cache: true
    pretraining_tp: 1
callbacks:
  model_checkpoint:
    save_top_k: 1

trainer:
  devices: 4
  target_tokens_per_batch: 400000
  tokens_per_document: 30000
  batch_size: ${data.batch_size}
  deterministic: false
  log_every_n_steps: 10
  timeout: 1800
  profiler:
    name: null
    log_tensorboard: false
    simple:
      _target_: SimpleProfiler
    advance:
      _target_: AdvancedProfiler
      filename: advanced_perf_logs
      dirpath: ./profiler_logs
    pytorch:
      _target_: PyTorchProfiler
      filename: pytorch_perf_logs
      dirpath: ./profiler_logs
      record_shapes: true
      profile_memory: true
      with_stack: true
      with_flops: false
      with_modules: false
      acc_events: false
  strategy: ddp
  num_nodes: 1
  sync_batchnorm: true
  precision: bf16-true
  min_epochs: 1000
  accumulate_grad_batches: null
  use_distributed_sampler: false
paths:
  root_dir: ${oc.env:PROJECT_ROOT}
  data_dir: ${paths.root_dir}/../ProFam-atlas
  log_dir: ${paths.root_dir}/logs/${experiment_group}
  output_dir: ${hydra:runtime.output_dir}
  work_dir: ${hydra:runtime.cwd}
  gym_data_dir: ../ProteinGym
extras:
  ignore_warnings: false
  enforce_tags: true
  print_config: true
tokenizer:
  _target_: src.data.tokenizers.ProFamTokenizer
  tokenizer_file: data/profam_tokenizer.json
  unk_token: '[UNK]'
  pad_token: '[PAD]'
  bos_token: '[start-of-document]'
  sep_token: '[SEP]'
  mask_token: '?'
  seq_struct_sep_token: '|'
  add_final_sep: true
  add_bos_token: true
  add_document_token: true
constants:
  vocab_size: 68
  gym_val_assay_list:
  - BLAT_ECOLX_Jacquier_2013
  - CALM1_HUMAN_Weile_2017
  - DYR_ECOLI_Thompson_2019
  - DLG4_RAT_McLaughlin_2012
  - REV_HV1H2_Fernandes_2016
  - TAT_HV1BR_Fernandes_2016
  - RL40A_YEAST_Roscoe_2013
  - P53_HUMAN_Giacomelli_2018_WT_Nutlin
  sequence_features:
  - ds_name
  - identifier
  - input_ids
  - attention_mask
  - original_size
  - batch_size
extra_callbacks: null

data:
  _target_: src.data.datamodule.ProteinDataMixture
  dataset_builders:
    openfold_train:
      _target_: src.data.builders.family_text_memmap_datasets.ProteinFamilyMemmapDataset
      name: openfold_train
      dataset_root: ${paths.data_dir}/OpenFold_OpenProteinSet/train
      tokenizer: ${tokenizer}
      preprocessor:
        _target_: src.data.processors.ProteinDocumentPreprocessor
        cfg:
          _target_: src.data.processors.AlignedProteinPreprocessingConfig
          document_token: '[RAW]'
          drop_first_protein: false
          keep_first_protein: false
          allow_unk: false
          max_tokens_per_example: 8192
          shuffle_proteins_in_document: true
          padding: do_not_pad
          keep_gaps: false
          keep_insertions: true
          to_upper: true
          use_msa_pos: false
        transform_fns: null
    proteingym:
      _target_: src.data.builders.proteingym.ProteinGymDataset
      name: proteingym
      dms_ids: ${constants.gym_val_assay_list}
      gym_data_dir: ${paths.gym_data_dir}
      seed: 42
      mutant_bos_token: sep
      keep_gaps: false
      use_filtered_msa: false
      extra_tokens_per_document: 2
      max_mutated_sequences: 1000
      use_msa_pos: false
      num_proc: null
      max_tokens_per_example: 7000
      max_context_seqs: null
      keep_wt: false
      drop_wt: true
      msa_folder_name: "PoET_DMS_msa_files/DMS_substitutions"
    foldseek_s50_train:
      _target_: src.data.builders.family_text_memmap_datasets.ProteinFamilyMemmapDataset
      name: foldseek_s50_train
      dataset_root: ${paths.data_dir}/foldseek_s50/train
      tokenizer: ${tokenizer}
      seed: ${seed}
      preprocessor:
        _target_: src.data.processors.ProteinDocumentPreprocessor
        cfg:
          _target_: src.data.processors.PreprocessingConfig
          document_token: '[RAW]'
          drop_first_protein: false
          keep_first_protein: false
          allow_unk: false
          max_tokens_per_example: 8192
          shuffle_proteins_in_document: true
          padding: do_not_pad
        transform_fns: null
    foldseek_s50_val:
      _target_: src.data.builders.family_text_memmap_datasets.ProteinFamilyMemmapDataset
      name: foldseek_s50_val
      dataset_root: ${paths.data_dir}/foldseek_s50/val
      tokenizer: ${tokenizer}
      seed: ${seed}
      preprocessor:
        _target_: src.data.processors.ProteinDocumentPreprocessor
        cfg:
          _target_: src.data.processors.PreprocessingConfig
          document_token: '[RAW]'
          drop_first_protein: false
          keep_first_protein: false
          allow_unk: false
          max_tokens_per_example: 8192
          shuffle_proteins_in_document: true
          padding: do_not_pad
        transform_fns: null
    uniref90_train:
      _target_: src.data.builders.family_text_memmap_datasets.ProteinFamilyMemmapDataset
      name: uniref90_train
      dataset_root: ${paths.data_dir}/uniref90/train
      tokenizer: ${tokenizer}
      preprocessor:
        _target_: src.data.processors.ProteinDocumentPreprocessor
        cfg:
          _target_: src.data.processors.PreprocessingConfig
          document_token: '[RAW]'
          drop_first_protein: false
          keep_first_protein: false
          allow_unk: false
          max_tokens_per_example: 320
          shuffle_proteins_in_document: true
          padding: do_not_pad
        transform_fns: null

    funfams_s50_train:
      _target_: src.data.builders.family_text_memmap_datasets.ProteinFamilyMemmapDataset
      name: funfams_s50_train
      dataset_root: ${paths.data_dir}/funfams_s50/train
      tokenizer: ${tokenizer}
      seed: ${seed}
      preprocessor:
        _target_: src.data.processors.ProteinDocumentPreprocessor
        cfg:
          _target_: src.data.processors.AlignedProteinPreprocessingConfig
          document_token: '[RAW]'
          drop_first_protein: false
          keep_first_protein: false
          allow_unk: false
          max_tokens_per_example: 8192
          shuffle_proteins_in_document: true
          padding: do_not_pad
          keep_gaps: false
          keep_insertions: true
          to_upper: true
          use_msa_pos: false
        transform_fns: null

  data_weights:
    foldseek_s50_train: 1
    uniref90_train: 1
    openfold_train: 1
    funfams_s50_train: 0.03
  val_dataset_batch_sizes:
    foldseek_s50_val: 1
    proteingym: 1
  batch_size: 100
  data_dir: ${paths.data_dir}
  num_workers: 32
  ignore_gaps: true
  feature_names: ${constants.sequence_features}
  pack_to_max_tokens: 52000
  prefetch_factor: 4
  shuffle: true
  interleaved: true
  interleaved_block_size: 1000
