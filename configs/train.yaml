# @package _global_
#
# Minimal Hydra setup:
# - `train.yaml` contains the full base config
# - `configs/experiment/*.yaml` contains small preset overrides
#
# Switch presets via CLI:
#   python src/train.py experiment=train_profam_example
defaults:
  - _self_
  - data: profam
  - experiment: null

# task name, determines output directory path
task_name: train
experiment_group: ProFam1
tags: ["${experiment_group}"]

train: true
test: false
ckpt_path: null
seed: 12345
float32_matmul_precision: high


hydra:
  run:
    dir: ${paths.log_dir}/runs/${now:%Y-%m-%d}_${now:%H-%M-%S-%f}
  sweep:
    dir: ${paths.log_dir}/multiruns/${now:%Y-%m-%d}_${now:%H-%M-%S-%f}
    subdir: ${hydra.job.num}
  job_logging:
    handlers:
      file:
        filename: ${hydra.runtime.output_dir}/${task_name}.log


paths:
  root_dir: ${oc.env:PROJECT_ROOT}
  # Default to the full dataset location
  data_dir: ${paths.root_dir}/../ProFam-atlas
  gym_data_dir: ${paths.data_dir}/ProteinGym
  log_dir: ${paths.root_dir}/logs/${experiment_group}
  output_dir: ${hydra:runtime.output_dir}
  work_dir: ${hydra:runtime.cwd}


extras:
  ignore_warnings: false
  enforce_tags: true
  print_config: true

constants:
  vocab_size: 68
  gym_val_assay_list:
    - BLAT_ECOLX_Jacquier_2013
    - CALM1_HUMAN_Weile_2017
    - DYR_ECOLI_Thompson_2019
    - DLG4_RAT_McLaughlin_2012
    - REV_HV1H2_Fernandes_2016
    - TAT_HV1BR_Fernandes_2016
    - RL40A_YEAST_Roscoe_2013
    - P53_HUMAN_Giacomelli_2018_WT_Nutlin
  sequence_features:
    - ds_name
    - identifier
    - input_ids
    - attention_mask
    - original_size
    - batch_size

tokenizer:
  _target_: src.data.tokenizers.ProFamTokenizer
  tokenizer_file: ${paths.root_dir}/data/profam_tokenizer.json
  unk_token: "[UNK]"
  pad_token: "[PAD]"
  bos_token: "[start-of-document]"
  sep_token: "[SEP]"
  mask_token: "?"
  seq_struct_sep_token: "|"
  add_final_sep: true
  add_bos_token: true
  add_document_token: true


model:
  _target_: src.models.llama.LlamaLitModule
  scheduler_name: constant_with_warmup
  num_warmup_steps: 1000
  num_training_steps: 1000000
  lr: 0.001
  use_kv_cache_for_scoring: true
  pass_res_pos_in_doc_as_position_ids: true
  optimizer: adamw
  config:
    _target_: transformers.LlamaConfig
    vocab_size: ${constants.vocab_size}
    hidden_size: 1024
    intermediate_size: 4096
    num_attention_heads: 16
    num_hidden_layers: 16
    num_key_value_heads: 8
    rope_theta: 500000
    max_position_embeddings: 8192
    scoring_max_tokens: 128000
    attn_implementation: flash_attention_2
    attention_bias: false
    attention_dropout: 0.0
    rms_norm_eps: 1.0e-05
    hidden_act: silu
    torch_dtype: bfloat16
    use_cache: true
    pretraining_tp: 1


callbacks:
  throughput:
    _target_: src.utils.callbacks.TokenThroughputMonitor
  model_checkpoint:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    dirpath: ${paths.output_dir}/checkpoints
    filename: epoch_{epoch:03d}
    monitor: val/loss
    verbose: false
    save_last: true
    save_top_k: 5
    mode: min
    auto_insert_metric_name: false
    save_weights_only: false
    every_n_train_steps: null
    train_time_interval: null
    every_n_epochs: null
    save_on_train_epoch_end: null
  model_summary:
    _target_: lightning.pytorch.callbacks.RichModelSummary
    max_depth: -1
  rich_progress_bar:
    _target_: lightning.pytorch.callbacks.RichProgressBar
  timer:
    _target_: src.utils.callbacks.EpochTimerCallback
  print:
    _target_: src.utils.callbacks.PrintCallback
  sample_counter:
    _target_: src.utils.callbacks.SampleCounter

extra_callbacks: null

logger:
  wandb:
    _target_: src.utils.loggers.WandbLogger
    save_dir: ${paths.output_dir}
    offline: false
    id: null
    anonymous: null
    project: profam
    log_model: false
    prefix: ""
    entity: ProFam
    group: ""
    name: null
    tags: ${tags}
    job_type: ""
    log_hydra_config_file: true
    log_git_hash: true


trainer:
  _target_: src.utils.trainer.ProFamTrainer
  default_root_dir: ${paths.output_dir}
  max_epochs: 10000
  max_steps: -1
  accelerator: gpu
  devices: auto
  check_val_every_n_epoch: 1
  val_check_interval: 50000
  target_tokens_per_batch: null
  tokens_per_document: 30000
  batch_size: ${data.batch_size}
  deterministic: false
  log_every_n_steps: 10
  timeout: 1800
  profiler:
    name: null
    log_tensorboard: false
    simple:
      _target_: SimpleProfiler

  strategy: ddp
  num_nodes: 1
  sync_batchnorm: true
  precision: bf16-true
  min_epochs: 1000
  accumulate_grad_batches: 2
  use_distributed_sampler: false
